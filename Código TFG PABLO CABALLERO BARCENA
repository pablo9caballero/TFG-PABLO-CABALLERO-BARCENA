# -*- coding: utf-8 -*-
"""Normalización - Desbalanceo y Regresión Logística - K-Means.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-mOYnLO0kRALWpCsDVhXRsRXfkTIwoBF
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# Cargar el dataset
file_path = "/content/final 2024_Accidentalidad-definito copia pablo caballero.xlsx"
df = pd.read_excel(file_path, engine="openpyxl")

# Importar librerías necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE

# Filtrar solo peatones (si es un criterio en el análisis)
df = df[df['tipo_peaton'] == 1]

# Variables predictoras y variable objetivo
X = df.drop(columns=['lesividad_binaria'])
y = df['lesividad_binaria']

# Aplicar SMOTE para balancear la muestra (igualar clases de lesividad_binaria)
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Crear el nuevo dataframe balanceado
df_balanced = pd.DataFrame(X_resampled, columns=X.columns)
df_balanced['lesividad_binaria'] = y_resampled

# Seleccionar solo variables **continuas** para normalización
continuous_cols = ['coordenada_x_utm', 'coordenada_y_utm']

# Aplicar normalización solo a las variables continuas
scaler = StandardScaler()
df_balanced[continuous_cols] = scaler.fit_transform(df_balanced[continuous_cols])

# Guardar dataset preprocesado
df_balanced.to_csv('dataset_preprocesado.csv', index=False)

# Visualización del balanceo
fig, ax = plt.subplots(1, 2, figsize=(12,5))
sns.countplot(x=y, ax=ax[0], palette='coolwarm')
ax[0].set_title('Distribución antes del balanceo')
sns.countplot(x=y_resampled, ax=ax[1], palette='coolwarm')
ax[1].set_title('Distribución después del balanceo')
plt.show()

# Visualización de la normalización solo de variables continuas
fig, ax = plt.subplots(1, 2, figsize=(16,6))
sns.boxplot(data=df[continuous_cols], ax=ax[0])
ax[0].set_title("Distribución de variables antes de normalización")
sns.boxplot(data=df_balanced[continuous_cols], ax=ax[1])
ax[1].set_title("Distribución de variables después de normalización")
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc

# Cargar el dataset preprocesado
df = pd.read_csv('dataset_preprocesado.csv')

### REGRESIÓN LOGÍSTICA ###

# Separar variables predictoras y variable objetivo
X = df.drop(columns=['lesividad_binaria'])
y = df['lesividad_binaria']

# Dividir en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Entrenar modelo de regresión logística
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
logistic_model.fit(X_train, y_train)

# Predicciones
y_pred = logistic_model.predict(X_test)
y_pred_prob = logistic_model.predict_proba(X_test)[:, 1]

# Evaluación del modelo
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Matriz de confusión
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues", xticklabels=["No Grave", "Grave"], yticklabels=["No Grave", "Grave"])
plt.xlabel("Predicho")
plt.ylabel("Real")
plt.title("Matriz de confusión - Regresión Logística")
plt.show()

# Curva ROC y AUC
fpr, tpr, _ = roc_curve(y_test, y_pred_prob)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6,4))
plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='grey', linestyle='--')
plt.xlabel("Tasa de falsos positivos")
plt.ylabel("Tasa de verdaderos positivos")
plt.title("Curva ROC - Regresión Logística")
plt.legend()
plt.show()

# Imprimir métricas de evaluación
print("Métricas de Regresión Logística:")
print(f"Accuracy: {accuracy:.2f}")
print(f"Precisión: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

import pandas as pd

# Cargar el dataset preprocesado
file_path = "dataset_preprocesado.csv"
df = pd.read_csv(file_path)

# Mostrar nombres de las columnas
print("Variables en el dataset:")
print(df.columns)

import pandas as pd

# Cargar el dataset preprocesado
df = pd.read_csv('dataset_preprocesado.csv')

# Guardar una muestra para verificar que todo está correcto
df.head()

# Reimportar librerías después del reinicio del entorno
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el dataset
file_path = "/content/final 2024_Accidentalidad-definito copia pablo caballero.xlsx"
df = pd.read_excel(file_path, engine="openpyxl")


# Filtrar solo atropellos donde el peatón es la víctima
df_peatones = df[df['tipo_peaton'] == 1]

# Agrupar por distrito y contar el número total de atropellos
df_distritos_accidentes = df_peatones.groupby('cod_distrito').size().reset_index(name='total_atropellos')

#  Generar gráfico corregido de distribución de atropellos por distrito
plt.figure(figsize=(12, 6))
sns.barplot(x=df_distritos_accidentes['cod_distrito'], y=df_distritos_accidentes['total_atropellos'], palette="viridis")
plt.xlabel("Código de distrito")
plt.ylabel("Número de atropellos")
plt.title("Distribución de atropellos por distrito")
plt.xticks(range(1, 22))
plt.show()



#  Importar librerías necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

#  Cargar la base de datos procesada
file_path_procesado = "/content/dataset_preprocesado.csv"
df_procesado = pd.read_csv(file_path_procesado)

# Normalizar solo coordenadas para que la escala no afecte el clustering
scaler = StandardScaler()
df_procesado[['coordenada_x_utm', 'coordenada_y_utm']] = scaler.fit_transform(df_procesado[['coordenada_x_utm', 'coordenada_y_utm']])

#  Seleccionar variables clave para clustering
vars_clustering = ['cod_distrito', 'coordenada_x_utm', 'coordenada_y_utm', 'lesividad_binaria']

#  Confirmar que los datos están correctamente cargados
df_procesado.head()

#  Importar librerías necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Cargar los datos
df = pd.read_excel("/content/final 2024_Accidentalidad-definito copia pablo caballero.xlsx")

# Filtrar solo peatones (excluir conductores)
df = df[df['tipo_peaton'] == 1]

# Calcular la proporción de accidentes graves por distrito
df_graves = df[df['lesividad_binaria'] == 1].groupby('cod_distrito').size().reset_index(name='num_graves')
df_total = df.groupby('cod_distrito').size().reset_index(name='num_total')
df_clusters = df_total.merge(df_graves, on='cod_distrito', how='left').fillna(0)
df_clusters['proporcion_graves'] = df_clusters['num_graves'] / df_clusters['num_total']

# Normalizar la proporción de accidentes graves
scaler = StandardScaler()
df_clusters['proporcion_graves_scaled'] = scaler.fit_transform(df_clusters[['proporcion_graves']])

# Método del Codo para determinar el mejor número de clusters
wcss = []
silhouette_scores = {}

for k in range(2, 6):  # Probar entre 2 y 5 clusters
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(df_clusters[['proporcion_graves_scaled']])

    wcss.append(kmeans.inertia_)  # Guardar inercia (para Elbow)
    silhouette_scores[k] = silhouette_score(df_clusters[['proporcion_graves_scaled']], labels)

# Graficar el método del codo (Elbow Method)
plt.figure(figsize=(8, 5))
plt.plot(range(2, 6), wcss, marker='o', linestyle='-', color='blue')
plt.xlabel("Número de clusters (K)")
plt.ylabel("Inercia (WCSS)")
plt.title("Método del Codo para Selección de K")
plt.grid()
plt.show()

# Graficar Silhouette Score para diferentes K
plt.figure(figsize=(8, 5))
plt.plot(list(silhouette_scores.keys()), list(silhouette_scores.values()), marker='o', linestyle='-', color='red')
plt.xlabel("Número de clusters (K)")
plt.ylabel("Silhouette Score")
plt.title("Silhouette Score para diferentes valores de K")
plt.grid()
plt.show()

# Aplicar K-Means con el mejor número de clusters
best_k = max(silhouette_scores, key=silhouette_scores.get)  # Se escoge el K con mejor Silhouette Score
kmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=10)
df_clusters['cluster'] = kmeans_final.fit_predict(df_clusters[['proporcion_graves_scaled']])

# Gráfico de barras: Segmentación de distritos por proporción de accidentes graves
plt.figure(figsize=(10, 5))
sns.barplot(x=df_clusters['cod_distrito'], y=df_clusters['proporcion_graves'], hue=df_clusters['cluster'], palette='coolwarm')
plt.xlabel("Código de distrito")
plt.ylabel("Proporción de accidentes graves")
plt.title("Segmentación de distritos según la proporción de accidentes graves")
plt.xticks(rotation=45)
plt.legend(title="Cluster")
plt.show()



# Comparación de factores de riesgo entre clusters con Boxplots
df_melted = df_clusters.melt(id_vars=['cluster'], value_vars=factor_vars)

plt.figure(figsize=(14, 6))
sns.boxplot(x='variable', y='value', hue='cluster', data=df_melted, palette="dark")
plt.xlabel("Factores de riesgo")
plt.ylabel("Distribución en clusters")
plt.title("Comparación de factores de riesgo entre clusters")
plt.legend(title="Cluster")
plt.xticks(rotation=45)
plt.show()
